{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews collected so far\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews collected so far\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews collected so far\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews collected so far\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews collected so far\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews collected so far\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews collected so far\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews collected so far\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews collected so far\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews collected so far\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Base URL and parameters\n",
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10  # Number of pages to scrape\n",
    "page_size = 100  # Number of reviews per page\n",
    "\n",
    "# List to store reviews\n",
    "reviews = []\n",
    "\n",
    "# Headers to mimic a real browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Loop through pages\n",
    "for i in range(1, pages + 1):\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Construct the URL for the current page\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    try:\n",
    "        # Send GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check for successful request\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            content = response.content\n",
    "            parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Extract reviews\n",
    "            for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "                reviews.append(para.get_text(strip=True))\n",
    "\n",
    "            print(f\"   ---> {len(reviews)} total reviews collected so far\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {i}. Status Code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping page {i}: {e}\")\n",
    "\n",
    "    # Pause between requests to avoid being blocked\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified|  Very good flight following an e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified|  An hour's delay due to late arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅Trip Verified|   I booked through BA because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅Trip Verified|   British airways lost bags in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅Trip Verified| The check in process and rewar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  Not Verified|  Very good flight following an e...\n",
       "1  Not Verified|  An hour's delay due to late arr...\n",
       "2  ✅Trip Verified|   I booked through BA because ...\n",
       "3  ✅Trip Verified|   British airways lost bags in...\n",
       "4  ✅Trip Verified| The check in process and rewar..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 1000 reviews to C:\\Users\\SURFACE\\Documents\\A\\Forage\\British Airways Reviews\\british_airways_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where you want to save the file\n",
    "save_dir = r\"C:\\Users\\SURFACE\\Documents\\A\\Forage\\British Airways Reviews\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "csv_file = os.path.join(save_dir, \"british_airways_reviews.csv\")\n",
    "\n",
    "# Save reviews to a CSV file\n",
    "try:\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Review\"])\n",
    "        for review in reviews:\n",
    "            writer.writerow([review])\n",
    "    print(f\"Successfully saved {len(reviews)} reviews to {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving to CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
